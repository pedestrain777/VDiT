model_name: "EDEN_DiT"
mixed_precision: "no"
cos_sim_mean: 0.990539
cos_sim_std: 0.0608093
vae_scaler: 0.179437
vae_shift: 0.770202
model_args:
  latent_dim: 16
  dim: 768
  num_heads: 12
  mlp_ratio: 4.0
  depth: 12
  qkv_bias: false
  attn_drop_rate: 0.
  proj_drop_rate: 0.
  use_xformers: true
vae_args:
  in_dim: 3
  out_dim: 3
  patch_size: 16
  hidden_dim: 768
  num_heads: 12
  mlp_ratio: 4.
  latent_dim: 16
  encoder_depth: 4
  decoder_depth: 4
  qkv_bias: false
  attn_drop_rate: 0.
  proj_drop_rate: 0.
  use_xformers: true
  add_attn_encoder: true
  add_attn_decoder: true
  add_attn_type: "temporal_attn"
dataset_name: "LAVIB"
val_dataset_name: "DAVIS"
dataset_args:
  LAVIB:
    split_name: "train"
    data_dir: datasets/LAVIB
    height: 256
    width: 448
    dur_list: [3, 5, 7]
    dur_weights: [1.0, 0.0, 0.0]
  DAVIS:
    data_dir: datasets/DAVIS
    height: 256
    width: 448
    is_val: true
dataloader:
  batch_size: 32
  shuffle: true
  drop_last: true
  num_workers: 8
val_dataloader:
  batch_size: 32
  num_workers: 8
output_dir: output # experiment outputs dir
global_seed: 0
train_args:
  pretrained_vae_path: /data/models/eden_checkpoint/eden_vae.pt  # pretrained vae ckpt path
  resume_from_ckpt: null  # pretrained dit ckpt path, if resume from pretrained dit
  epochs: 100
  base_lr: 1.25e-05
  optimizer:
    weight_decay: 1.0e-4
    betas: [0.9, 0.99]
    eps: 1.0e-06
  lr_scheduler: "cosine"
  warmup_steps: 100
  log_every_steps: 10
  ckpt_every_steps: 2000
  visual_every_steps: 2000
  visual_results_num: 4
  val_every_steps: 2000
  val_metric: "PSNR"
transport:
  path_type: Linear
  prediction: velocity
  sample_eps: null
  train_eps: null
  loss_weight: null