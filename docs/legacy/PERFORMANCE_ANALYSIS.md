# 双GPU模式性能分析

## 📊 性能数据对比

### 视频信息
- **帧率**: 8 fps
- **时长**: 6 秒
- **总帧数**: 48 帧
- **需要处理的帧对**: 47 对

### 单GPU模式 vs 双GPU模式

| 指标 | 单GPU模式 | 双GPU模式 | HTTP云-边缘（client+2服务） |
|------|-----------|-----------|---------------------------|
| **总耗时 (real)** | 10.439秒 | 12.593秒 | 62.52秒 |
| **CPU用户时间 (user)** | 67.47秒 | 89.97秒 | 74.01秒 |
| **系统时间 (system)** | 12.72秒 | 13.40秒 | 13.94秒 |
| **CPU使用率** | 768% | 820% | 140% |
| **平均每对帧耗时** | 0.222秒 | 0.268秒 | 1.331秒 |
| **处理速度** | 4.5 对/秒 | 3.7 对/秒 | 0.75 对/秒 |

> HTTP 数据来自 `time python client.py --video_path examples/0.mp4 --output_dir interpolation_outputs/http_client_test`，终端输出：`74.01s user 13.94s system 140% cpu 1:02.52 total`。

## 🔍 性能分析

### 1. 总耗时变化来源

- **双GPU vs 单GPU**：慢约 **20.6%**，原因仍是打包/解包与串行执行（详见原分析）。
- **HTTP vs 双GPU**：慢约 **5×**（62.5s 对 12.6s），新增瓶颈包括 PNG 编解码、Base64 序列化、HTTP 往返与三进程间上下文切换。

#### ✅ 额外开销
1. **打包/解包开销** (每对帧)
   - `pack_enc_out()`: tensor从GPU移到CPU，序列化
   - `unpack_enc_out()`: 从bytes加载，tensor移到GPU
   - 每对帧都有这个开销

2. **GPU间数据传输**
   - `difference`从cuda:0传到cuda:1
   - `generated_frame`从cuda:1移回CPU
   - 多次GPU间数据传输

3. **模型加载开销**
   - 需要加载两个完整的EDEN模型
   - 显存占用翻倍

#### ⚠️ 未实现的优化
1. **串行执行**：encoder和DiT+decoder是串行的，没有并行
   - 理想情况：encoder处理下一对时，DiT+decoder处理当前对
   - 当前实现：必须等encoder完成才能开始DiT+decoder

2. **打包/解包效率**
   - 每对帧都打包/解包，没有批量处理
   - 可以优化为批量打包

### 2. CPU使用率分析

- **单GPU**: 768% ≈ 7.7 核（GPU重负载）。
- **双GPU**: 820% ≈ 8.2 核，额外 CPU 在打包/传输。
- **HTTP**: 140% ≈ 1.4 核，主耗时落在 I/O、编码以及 Python 解释器，GPU 两边多数时间在等待 HTTP 响应。

### 3. 每对帧耗时对比

| 模式 | 每对帧耗时 | 主要瓶颈 |
|------|-----------|----------|
| 单GPU | 0.222秒 | 纯GPU推理 |
| 双GPU | 0.268秒 | pack/unpack + GPU间传输 |
| HTTP  | 1.331秒 | PNG & Base64 编解码、HTTP 往返、三进程同步 |

## 💡 性能优化建议

### 1. 流水线并行（Pipeline Parallelism）

当前是串行执行：
```
帧对1: encode → pack → unpack → denoise+decode
帧对2: encode → pack → unpack → denoise+decode
...
```

优化为流水线：
```
时间轴:
t1: 帧对1 encode
t2: 帧对1 pack+unpack | 帧对2 encode
t3: 帧对1 denoise+decode | 帧对2 pack+unpack | 帧对3 encode
...
```

**预期提升**: 可减少20-30%的总耗时

### 2. 批量打包/解包

当前每对帧都单独打包/解包，可以：
- 收集多对帧的encoder输出
- 批量打包
- 批量解包

**预期提升**: 减少打包/解包开销

### 3. 异步传输

使用CUDA streams实现异步传输：
- encoder计算和打包可以并行
- 解包和denoise可以并行

**预期提升**: 减少等待时间

### 4. 减少数据传输

- `difference`很小，可以包含在打包数据中
- 减少单独的GPU间传输

## 📈 实际场景分析

### 当前实现的意义

虽然双GPU模式比单GPU慢，但它的价值在于：

1. **模拟真实场景**
   - 打包/解包模拟了网络传输
   - 为后续HTTP服务做准备

2. **资源分离**
   - encoder和DiT+decoder可以独立扩展
   - 适合云端-边缘架构

3. **负载均衡**
   - 两张GPU分担计算负载
   - 单张GPU显存压力减小

### 性能对比总结

| 场景 | 单GPU | 双GPU | 推荐 |
|------|-------|-------|------|
| **本地推理** | ✅ 更快 | ⚠️ 更慢 | 单GPU |
| **云端-边缘架构** | ❌ 不支持 | ✅ 支持 | 双GPU |
| **显存受限** | ⚠️ 压力大 | ✅ 分担负载 | 双GPU |
| **开发测试** | ✅ 简单 | ✅ 模拟真实 | 双GPU |

## 🎯 结论

1. **当前性能**: 双GPU模式比单GPU慢约20.6%
2. **主要原因**: 打包/解包开销和串行执行
3. **优化空间**: 流水线并行可显著提升性能
4. **实际价值**: 为云端-边缘架构做好准备

## 📝 下一步优化

1. ⏳ 实现流水线并行
2. ⏳ 优化打包/解包效率
3. ⏳ 实现异步传输
4. ⏳ 测试HTTP服务性能

